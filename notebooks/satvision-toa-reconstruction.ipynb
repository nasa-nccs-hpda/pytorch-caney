{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ab2075-c488-46b9-8cd2-0cdaf399acfc",
   "metadata": {},
   "source": [
    "# Satvision-TOA Reconstruction Notebook\n",
    "\n",
    "Version: 04.30.24\n",
    "\n",
    "Env: `Python [conda env:ilab-pytorch]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88ea70-7dbf-4b67-a12d-db36e2bc9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yacs timm segmentation-models-pytorch termcolor webdataset==0.2.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046c3e5-c458-4e03-9c96-e9eb95a04963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7db1bc-09ee-47e3-9015-e6b148d497e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../pytorch-caney')\n",
    "\n",
    "from pytorch_caney.config import get_config\n",
    "\n",
    "from pytorch_caney.training.mim_utils import load_checkpoint, load_pretrained\n",
    "\n",
    "from pytorch_caney.models.build import build_model\n",
    "\n",
    "from pytorch_caney.ptc_logging import create_logger\n",
    "\n",
    "from pytorch_caney.data.datamodules import mim_webdataset_datamodule\n",
    "\n",
    "from pytorch_caney.data.transforms import SimmimTransform, SimmimMaskGenerator\n",
    "\n",
    "from pytorch_caney.config import _C, _update_config_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841e464-f880-4e53-bf31-f9f225713918",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274e323-bc04-41d4-bc49-baed65d027e6",
   "metadata": {},
   "source": [
    "### Clone model ckpt from huggingface\n",
    "\n",
    "```bash\n",
    "# On prism/explore\n",
    "module load git-lfs\n",
    "\n",
    "git lfs install\n",
    "\n",
    "git clone git clone git@hf.co:nasa-cisto-data-science-group/satvision-toa-huge-patch8-window12-192\n",
    "```\n",
    "\n",
    "Note: If using git w/ ssh, make sure you have ssh keys enabled to clone using ssh auth.\n",
    "https://huggingface.co/docs/hub/security-git-ssh\n",
    "\n",
    "```bash\n",
    "eval $(ssh-agent)\n",
    "\n",
    "# If this outputs as anon, follow the next steps.\n",
    "ssh -T git@hf.co\n",
    "\n",
    "# Check if ssh-agent is using the proper key\n",
    "ssh-add -l\n",
    "\n",
    "# If not\n",
    "ssh-add ~/.ssh/your-key\n",
    "\n",
    "# Or if you want to use the default id_* key, just do\n",
    "ssh-add\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af699ba3-2d98-4daf-9437-c322d7b59a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH: str = '../../satvision-toa-huge-patch8-window12-192/mp_rank_00_model_states.pt'\n",
    "CONFIG_PATH: str = '../../satvision-toa-huge-patch8-window12-192/mim_pretrain_swinv2_satvision_huge_192_window12_100ep.yaml'\n",
    "\n",
    "OUTPUT: str = '.'\n",
    "TAG: str = 'satvision-huge-toa-reconstruction'\n",
    "DATA_PATH: str = '/explore/nobackup/projects/ilab/projects/3DClouds/data/validation/sv_toa_128_chip_validation_04_24.npy'\n",
    "DATA_PATHS: list = [DATA_PATH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4593e8c-6e94-4d01-b86e-5b78b621fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config given configurations\n",
    "\n",
    "config = _C.clone()\n",
    "_update_config_from_file(config, CONFIG_PATH)\n",
    "\n",
    "config.defrost()\n",
    "config.MODEL.RESUME = MODEL_PATH\n",
    "config.DATA.DATA_PATHS = DATA_PATHS\n",
    "config.OUTPUT = OUTPUT\n",
    "config.TAG = TAG\n",
    "config.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a4474-88e4-44d5-b899-7aaf6cbed6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='app.log',  # Specify the log file name\n",
    "    level=logging.INFO,  # Set logging level to DEBUG\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',  # Specify log message format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'  # Specify date format\n",
    ")\n",
    "\n",
    "# Add logging to standard output\n",
    "console = logging.StreamHandler()  # Create a handler for standard output\n",
    "console.setLevel(logging.INFO)  # Set logging level for standard output\n",
    "console.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))  # Set log message format for standard output\n",
    "logger = logging.getLogger('')\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebd497-7741-41a7-af9d-0ee49a6313a4",
   "metadata": {},
   "source": [
    "## 2. Load model weights from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abf348-c6bf-43a3-b00a-cc5f8d80545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(MODEL_PATH)\n",
    "model = build_model(config, pretrain=True)\n",
    "model.load_state_dict(checkpoint['module']) # If 'module' not working, try 'model'\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"number of params: {n_parameters}\")\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b500d13b-89d7-4cd8-a36a-ab6f10f6a397",
   "metadata": {},
   "source": [
    "## 3. Load evaluation set (from numpy file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8d307-de9b-4617-abdd-dae1e7c2521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Masked-Image-Modeling transform\n",
    "transform = SimmimTransform(config)\n",
    "\n",
    "# The reconstruction evaluation set is a single numpy file\n",
    "validation_dataset_path = config.DATA.DATA_PATHS[0]\n",
    "validation_dataset = np.load(validation_dataset_path)\n",
    "len_batch = range(validation_dataset.shape[0])\n",
    "\n",
    "# Apply transform to each image in the batch\n",
    "# A mask is auto-generated in the transform\n",
    "imgMasks = [transform(validation_dataset[idx]) for idx \\\n",
    "    in len_batch]\n",
    "\n",
    "# Seperate img and masks, cast masks to torch tensor\n",
    "img = torch.stack([imgMask[0] for imgMask in imgMasks])\n",
    "mask = torch.stack([torch.from_numpy(imgMask[1]) for \\\n",
    "    imgMask in imgMasks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55acf5e9-eb2a-496c-baa6-3b74503a2978",
   "metadata": {},
   "source": [
    "## 4. Prediction helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595336f8-71b4-418b-b153-2461583ed613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, num_batches=5):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    masks = []\n",
    "    losses = []\n",
    "    with tqdm(total=num_batches) as pbar:\n",
    "\n",
    "        for idx, img_mask in enumerate(dataloader):\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "            if idx > num_batches:\n",
    "                return inputs, outputs, masks, losses\n",
    "\n",
    "            img_mask = img_mask[0]\n",
    "\n",
    "            img = torch.stack([pair[0] for pair in img_mask])\n",
    "            mask = torch.stack([pair[1] for pair in img_mask])\n",
    "\n",
    "            img = img.cuda(non_blocking=True)\n",
    "            mask = mask.cuda(non_blocking=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with amp.autocast(enabled=config.ENABLE_AMP):\n",
    "                    z = model.encoder(img, mask)\n",
    "                    img_recon = model.decoder(z)\n",
    "                    loss = model(img, mask)\n",
    "\n",
    "            inputs.extend(img.cpu())\n",
    "            masks.extend(mask.cpu())\n",
    "            outputs.extend(img_recon.cpu())\n",
    "            losses.append(loss.cpu())\n",
    "    \n",
    "    return inputs, outputs, masks, losses\n",
    "\n",
    "\n",
    "def minmax_norm(img_arr):\n",
    "    arr_min = img_arr.min()\n",
    "    arr_max = img_arr.max()\n",
    "    img_arr_scaled = (img_arr - arr_min) / (arr_max - arr_min)\n",
    "    img_arr_scaled = img_arr_scaled * 255\n",
    "    img_arr_scaled = img_arr_scaled.astype(np.uint8)\n",
    "    return img_arr_scaled\n",
    "\n",
    "\n",
    "def process_mask(mask):\n",
    "    mask_img = mask.unsqueeze(0)\n",
    "    mask_img = mask_img.repeat_interleave(4, 1).repeat_interleave(4, 2).unsqueeze(1).contiguous()\n",
    "    mask_img = mask_img[0, 0, :, :]\n",
    "    mask_img = np.stack([mask_img, mask_img, mask_img], axis=-1)\n",
    "    return mask_img\n",
    "\n",
    "\n",
    "def process_prediction(image, img_recon, mask, rgb_index):\n",
    "\n",
    "    mask = process_mask(mask)\n",
    "    \n",
    "    red_idx = rgb_index[0]\n",
    "    blue_idx = rgb_index[1]\n",
    "    green_idx = rgb_index[2]\n",
    "\n",
    "    image = image.numpy()\n",
    "    rgb_image = np.stack((image[red_idx, :, :],\n",
    "                          image[blue_idx, :, :],\n",
    "                          image[green_idx, :, :]),\n",
    "                         axis=-1)\n",
    "    rgb_image = minmax_norm(rgb_image)\n",
    "\n",
    "    img_recon = img_recon.numpy()\n",
    "    rgb_image_recon = np.stack((img_recon[red_idx, :, :],\n",
    "                                img_recon[blue_idx, :, :],\n",
    "                                img_recon[green_idx, :, :]),\n",
    "                                axis=-1)\n",
    "    rgb_image_recon = minmax_norm(rgb_image_recon)\n",
    "\n",
    "    rgb_masked = np.where(mask == 0, rgb_image, rgb_image_recon)\n",
    "    rgb_image_masked = np.where(mask == 1, 0, rgb_image)\n",
    "    rgb_recon_masked = rgb_masked\n",
    "    \n",
    "    return rgb_image, rgb_image_masked, rgb_recon_masked, mask\n",
    "\n",
    "\n",
    "def plot_export_pdf(path, inputs, outputs, masks, rgb_index):\n",
    "    pdf_plot_obj = PdfPages(path)\n",
    "\n",
    "    for idx in range(len(inputs)):\n",
    "        # prediction processing\n",
    "        image = inputs[idx]\n",
    "        img_recon = outputs[idx]\n",
    "        mask = masks[idx]\n",
    "        rgb_image, rgb_image_masked, rgb_recon_masked, mask = \\\n",
    "            process_prediction(image, img_recon, mask, rgb_index)\n",
    "\n",
    "        # matplotlib code\n",
    "        fig, (ax01, ax23) = plt.subplots(2, 2, figsize=(40, 30))\n",
    "        ax0, ax1 = ax01\n",
    "        ax2, ax3 = ax23\n",
    "        ax2.imshow(rgb_image)\n",
    "        ax2.set_title(f\"Idx: {idx} MOD021KM v6.1 Bands: {rgb_index}\")\n",
    "\n",
    "        ax0.imshow(rgb_recon_masked)\n",
    "        ax0.set_title(f\"Idx: {idx} Model reconstruction\")\n",
    "\n",
    "        ax1.imshow(rgb_image_masked)\n",
    "        ax1.set_title(f\"Idx: {idx} MOD021KM Bands: {rgb_index}, masked\")\n",
    "        \n",
    "        ax3.matshow(mask[:, :, 0])\n",
    "        ax3.set_title(f\"Idx: {idx} Reconstruction Mask\")\n",
    "        pdf_plot_obj.savefig()\n",
    "\n",
    "    pdf_plot_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c44b5-6d88-45c4-b397-c38de8064544",
   "metadata": {},
   "source": [
    "## 5. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e695cc3-b869-4fc2-b360-b45f3b81affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "masks = []\n",
    "losses = []\n",
    "\n",
    "# We could do this in a single batch however we\n",
    "# want to report the loss per-image, in place of\n",
    "# loss per-batch.\n",
    "for i in tqdm(range(img.shape[0])):\n",
    "    single_img = img[i].unsqueeze(0)\n",
    "    single_mask = mask[i].unsqueeze(0)\n",
    "    single_img = single_img.cuda(non_blocking=True)\n",
    "    single_mask = single_mask.cuda(non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model.encoder(single_img, single_mask)\n",
    "        img_recon = model.decoder(z)\n",
    "        loss = model(single_img, single_mask)\n",
    "\n",
    "    inputs.extend(single_img.cpu())\n",
    "    masks.extend(single_mask.cpu())\n",
    "    outputs.extend(img_recon.cpu())\n",
    "    losses.append(loss.cpu()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f102c-94df-4d9e-8040-52197a7e71db",
   "metadata": {},
   "source": [
    "## 6. Plot and write to PDF\n",
    "\n",
    "Writes out all of the predictions to a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdcd1d-09db-4ccf-8cc1-58d6f47e3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = '../../satvision-toa-reconstruction-pdf-huge-patch-8-04.30.pdf'\n",
    "rgb_index = [0, 2, 1] # Indices of [Red band, Blue band, Green band]\n",
    "\n",
    "plot_export_pdf(pdf_path, inputs, outputs, masks, rgb_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a57f4d-5df0-47a3-bfb6-d7f29a95e276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ilab-pytorch]",
   "language": "python",
   "name": "conda-env-ilab-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
